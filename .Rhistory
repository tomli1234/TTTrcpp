install.packages('Rcpp')
library(TTTrcpp)
rcpp_hello()
library(TTTrcpp)
which_equal_C(1:3,1)
which_equal_C(1:3,2)
library(TTTrcpp)
timesTwo()
library(TTTrcpp)
check_which_state_2_C
library(TTTrcpp)
which_equal_C
which_equal_C{1:3,1_}
which_equal_C(1:3,2)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
rotate_90_C(matrix(1:900, 30, 30))
library(TTTrcpp)
rotate_90_C(matrix(1:900, 30, 30))
remove.packages(TTTrcpp)
remove.packages('TTTrcpp')
library(TTTrcpp)
rotate_90_C(matrix(1:900, 30, 30))
library(TTTrcpp)
rotate_90_C(matrix(1:900, 30, 30))
library(TTTrcpp)
rotate_90_C
rotate_90_C(matrix(1:900, 30, 30))
which_equal_C(1:3, 2)
which_equal_C
library(TTTrcpp)
rotate_90_C
rotate_90_C(matrix(1:900, 30, 30))
rotate_90_C
library(TTTrcpp)
rotate_90_C(matrix(1:900, 30, 30))
rotate_90_C
which_equal_C
learn_progress_C
library(TTTrcpp)
learn_progress_C
which_equal_C(1:10,4)
rotate_90_C
rotate_90_C(matrix(1:900, 30, 30))
rotate_90_C(matrix(1:900, 30, 30))
rotate_90_C(matrix(1:900, 30, 30))
library(TTTrcpp)
reflect_row_C(matrix(1:9, 3, 3))
library(TTTrcpp)
reflect_row_C(matrix(1:9, 3, 3))
rotate_90_C(matrix(1:9, 3, 3))
rotate_90_C(rotate_90_C(matrix(1:9, 3, 3)))
rotate_90_C(reflect_row_C(matrix(1:9, 3, 3)))
library(TTTrcpp)
check_status_C
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
library(TTTrcpp)
check_status_C
#----------------------
# An R program to implement the Tic-Tac-Toe example in
# Reinforcement Learning: An Introduction (Sutton and Barto)
#----------------------
rm(list=ls())
library(microbenchmark)
library(Rcpp)
library(TTTrcpp)
possible_move <- function(current_state,
turn){
possible_decision <- which_equal_C(current_state, -1)
sapply(possible_decision, function(x) {
current_state[x] <- turn
return(state = current_state)
}
)
}
sample.vec <- function(x, ...) x[sample(length(x), ...)]
## Initialisation
learning <- function(alpha = 0.1, random = 0.1,
rounds,
learned_state = NULL) {
if(is.null(learned_state)) {
learned_state <- NULL
learned_state[[1]] <- matrix(c(rep(-1, 25), 0.5), 1, 26)
learned_state[[2]] <- matrix(c(rep(-1, 25), 0.5), 1, 26)
}
progress <- NULL
## Learning
for(i in 1:rounds){
# alpha <- 1/i^(1/2.5)
current_state <- rep(-1,25)
turn <- sample(0:1, 1)
backup_state <- list(matrix(-2, ncol = 25),matrix(-2, ncol = 25))
while(check_status_C(current_state, turn) == -1){
## Update experience--------------
x <- t(possible_move(current_state, turn = turn))
learned	<- check_which_state_2_C(as.matrix(learned_state[[1 + turn]][, 1:25]), x)
option <- learned[learned!=0]
# If not seen possible move, then assign it with 0.5
if(sum(learned == 0) > 0){
option <- c(option, nrow(learned_state[[1 + turn]]) + 1:sum(learned == 0))
learned_state[[1 + turn]] <- rbind(learned_state[[1 + turn]],
cbind(matrix(x[learned == 0, ],
nrow=sum(learned == 0)), 0.5))
}
## Decision----------------------
# option	<- check_which_state_2_C(as.matrix(learned_state[[1 + turn]][, 1:25]), x)
decision_values <- learned_state[[1 + turn]][option, 26]
random_move <- runif(1) < random
if(random_move){
which_option <- sample(option, 1)
} else {
which_option <- option[sample.vec(which_equal_C(decision_values, max(decision_values)), 1)]
}
decision <- learned_state[[1 + turn]][which_option, ]
last_move <- check_which_state_2_C(as.matrix(learned_state[[1 + turn]][, 1:25]), matrix(backup_state[[1 + turn]], ncol = 25))
old_value <- learned_state[[1 + turn]][last_move, 26]
current_state <- decision[1:25]
current_status <- check_status_C(current_state, turn)
## Learning---------------------
### Current move
if(current_status == -1){
new_value <- decision[26]
learned_state[[1 + turn]][last_move, 26] <- old_value + alpha * (new_value - old_value)
} else {
new_value <- current_status
learned_state[[1 + turn]][last_move, 26] <- old_value + alpha * (new_value - old_value)
learned_state[[1 + turn]][which_option, 26] <- new_value
}
backup_state[[1 + turn]] <- current_state
turn <- abs(turn - 1)
### Learning from opponent's move (learning defensive move)
oppo_state <- check_which_state_2_C(as.matrix(learned_state[[1 + turn]][, 1:25]), matrix(backup_state[[1 + turn]], ncol = 25))
oppo_value <- learned_state[[1 + turn]][oppo_state, 26]
oppo_status <- check_status_C(current_state, turn)
if(oppo_status == -1){
# new_value <- decision[10]
# learned_state[[1 + turn]][oppo_state, 10] <- oppo_value + alpha * (new_value - oppo_value)
} else {
new_value <- oppo_status
learned_state[[1 + turn]][oppo_state, 26] <- oppo_value + alpha * (new_value - oppo_value)
# print(learned_state[[1 + turn]][oppo_state, 10] )
}
}
print(paste0(i,', ', nrow(learned_state[[1]])))
progress <- c(progress, learn_progress_C(learned_state[[1]][,26]))
plot(progress, type='l')
}
return(learned_state)
}
microbenchmark(
# learners <- shadow_clone(learner_num = 3, sub_rounds = 100),
learner_2 <- learning(rounds = 20, learned_state = NULL),
times = 5)
library(TTTrcpp)
microbenchmark(
# learners <- shadow_clone(learner_num = 3, sub_rounds = 100),
learner_2 <- learning(rounds = 20, learned_state = NULL),
times = 5)
